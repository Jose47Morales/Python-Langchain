{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef0b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d602a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cargamos las variables de entorno\n",
    "load_dotenv(find_dotenv(), override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee534e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cargar_documento(archivo):\n",
    "    #Cargamos un documento PDF o DOCX y lo devolvemos en un formato procesable\n",
    "    nombre, extension = os.path.splitext(archivo)\n",
    "    if extension == '.pdf':\n",
    "        print(f'Cargando {archivo}...')\n",
    "        loader = PyPDFLoader(archivo)\n",
    "    elif extension == '.docx':\n",
    "        print(f'Cargando {archivo}...')\n",
    "        loader = Docx2txtLoader(archivo)\n",
    "    else:\n",
    "        raise ValueError('El formato del documento no es soportado.')\n",
    "    return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def desde_wikipedia(busqueda, lang='es', load_max_docs=2):\n",
    "    # Realizamos una búsqueda en Wikipedia y retornamos los resultados.\n",
    "    loader = WikipediaLoader(query=busqueda, lang=lang, load_max_docs=load_max_docs)\n",
    "    return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c58339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fragmentar(data, chunk_size=150, chunk_overlap=20):\n",
    "    # Dividimos el texto en fragmentos más pequeños para el procesamiento\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c22dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def costo_embedding(texts):\n",
    "    # Calculamos el costo de embeddings en función del número de tokens.\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum(len(enc.encode(page.page_content)) for page in texts)\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0001:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f94ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def creando_vectores(fragmentos, index_name):\n",
    "    import time\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    from langchain.schema import Document\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    pc = Pinecone(api_key = os.environ.get('PINECONE_API_KEY'))\n",
    "    \n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    if index_name not in existing_indexes:\n",
    "        print(f'Creando el indice {index_name} y los embeddings ...', end = '')\n",
    "        pc.create_index(name = index_name,\n",
    "                        dimension = 1536, \n",
    "                        metric = 'cosine',\n",
    "                        spec = ServerlessSpec(cloud = \"aws\", region = \"us-east-1\"),\n",
    "                        )\n",
    "        while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "        vectores = Pinecone.from_documents(fragmentos, embeddings, index_name = index_name)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'El indice {index_name} ya existe. Cargando los embeddings ...', end = '')\n",
    "        vectores = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "\n",
    "    return vectores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e6e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consultas(vectores, pregunta):\n",
    "    # Realizamos consultas utilizando vectores y un modelo LLM.\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vectores.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    return chain.run(pregunta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77886f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consulta_con_memoria(vectores, pregunta, memoria=[]):\n",
    "    # Realizamos consultas con memoria de conversaciones previas.\n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vectores.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    respuesta = crc({'question': pregunta, 'chat_history': memoria})\n",
    "    memoria.append((pregunta, respuesta['answer']))\n",
    "    return respuesta, memoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5f4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def borrar_indices(index_name='todos'):\n",
    "    from pinecone import Pinecone\n",
    "    # Borramos índices de Pinecone según el nombre especificado.\n",
    "    pinecone_client = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "    existing_indexes = [index[\"name\"] for index in pinecone_client.list_indexes()]\n",
    "    if index_name == 'todos':\n",
    "        print('Borrando todos los índices ...')\n",
    "        for index in existing_indexes:\n",
    "            pinecone_client.delete_index(index)\n",
    "        print('Listo!')\n",
    "    elif index_name in existing_indexes:\n",
    "        print(f'Borrando el índice {index_name} ...', end='')\n",
    "        pinecone_client.delete_index(index_name)\n",
    "        print('Listo')\n",
    "    else:\n",
    "        print(f'El índice {index_name} no existe.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0c9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Criptografía cuántica.pdf...\n",
      "El número de fragmentos es de: 144 fragmentos\n",
      "Total Tokens: 3660\n",
      "Embedding Cost in USD: 0.00037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josem\\Documents\\Python-Langchain\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borrando todos los índices ...\n",
      "Listo!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josem\\AppData\\Local\\Temp\\ipykernel_3056\\3445364043.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando el indice criptografia-cuantica y los embeddings ...Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josem\\AppData\\Local\\Temp\\ipykernel_3056\\2109225436.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  respuesta = crc({'question': pregunta, 'chat_history': memoria})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una de las propiedades más importantes de la criptografía cuántica es la imposibilidad de conocer el valor exacto de dos propiedades complementarias al mismo tiempo.\n",
      "La imposibilidad de conocer el valor exacto de dos propiedades complementarias al mismo tiempo en la criptografía cuántica se propuso inicialmente en 1984.\n",
      "Adios!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ejemplo de uso\n",
    "documento = \"Criptografía cuántica.pdf\"\n",
    "contenido = cargar_documento(documento)\n",
    "fragmentos = fragmentar(contenido)\n",
    "print(f\"El número de fragmentos es de: {len(fragmentos)} fragmentos\")\n",
    "costo_embedding(fragmentos)\n",
    "borrar_indices(\"todos\")\n",
    "index_name = 'criptografia-cuantica'\n",
    "vectores = creando_vectores(fragmentos, index_name)\n",
    "\n",
    "# Ciclo de preguntas\n",
    "memoria = []\n",
    "while True:\n",
    "    pregunta = input(\"Realiza una pregunta, escribe 'salir' para terminar: \\n\")\n",
    "    if pregunta.lower() == \"salir\":\n",
    "        print(\"Adios!!!\")\n",
    "        break\n",
    "    else:\n",
    "        respuesta, memoria = consulta_con_memoria(vectores, pregunta, memoria)\n",
    "        print(respuesta['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
